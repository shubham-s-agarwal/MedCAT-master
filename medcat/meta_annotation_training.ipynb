{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import date\n",
    "from medcat.cat import CAT\n",
    "from medcat.meta_cat import MetaCAT\n",
    "from medcat.config_meta_cat import ConfigMetaCAT\n",
    "from medcat.tokenizers.meta_cat_tokenizers import TokenizerWrapperBPE, TokenizerWrapperBERT\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative path to working_with_cogstack folder\n",
    "_rel_path = os.path.join(\"..\", \"..\", \"..\")\n",
    "# absolute path to working_with_cogstack folder\n",
    "base_path = os.path.abspath(_rel_path)\n",
    "# Load mct export\n",
    "ann_dir = os.path.join(base_path, \"data\", \"medcattrainer_export\")\n",
    "\n",
    "mctrainer_export_path = ann_dir + \"\"  # name of your mct export\n",
    "\n",
    "# Load model\n",
    "model_dir = os.path.join(base_path, \"models\", \"modelpack\")\n",
    "modelpack = '' # name of modelpack\n",
    "model_pack_path = os.path.join(model_dir, modelpack)\n",
    "     #output_modelpack = model_dir + f\"{today}_trained_model\"\n",
    "\n",
    "# will be used to date the trained model\n",
    "today = str(date.today())\n",
    "today = today.replace(\"-\",\"\")\n",
    "\n",
    "# Initialise meta_ann models\n",
    "if model_pack_path[-4:] == '.zip':\n",
    "    base_dir_meta_models = model_pack_path[:-4]\n",
    "else:\n",
    "    base_dir_meta_models = model_pack_path\n",
    "\n",
    "# Iterate through the meta_models contained in the model\n",
    "meta_model_names = [] # These Meta_annotation tasks should correspond to the ones labelled in the mcttrainer export\n",
    "for dirpath, dirnames, filenames in os.walk(base_dir_meta_models):\n",
    "    for dirname in dirnames:\n",
    "        if dirname.startswith('meta_'):\n",
    "            meta_model_names.append(dirname[5:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you run the next section please double check that the model meta_annotation names matches to those specified in the mct export."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_model in meta_model_names:\n",
    "    vocab_file = os.path.join(base_dir_meta_models,\"meta_\"+meta_model,'bbpe-vocab.json')\n",
    "    merges_file = os.path.join(base_dir_meta_models,\"meta_\"+meta_model,'bbpe-merges.txt')\n",
    "    tokenizer = TokenizerWrapperBPE(ByteLevelBPETokenizer(vocab=vocab_file,\n",
    "                                    merges=merges_file,\n",
    "                                    lowercase=True))\n",
    "    # load and sort out the config\n",
    "    config_file = os.path.join(base_dir_meta_models,\"meta_\"+meta_model,\"config.json\")\n",
    "    with open(config_file, 'r') as jfile:\n",
    "        config_dict = json.load(jfile)\n",
    "    config = ConfigMetaCAT()\n",
    "    for key, value in config_dict.items():\n",
    "        setattr(config, key, value['py/state']['__dict__'])\n",
    "        # Reset the config attributes. TODO: Talk to Mart about how his new config style has affected this and best practise going forward\n",
    "\n",
    "    save_dir_path= \"test_meta_\"+meta_model # Where to save the meta_model and results. \n",
    "    #Ideally this should replace the meta_models inside the modelpack\n",
    "\n",
    "    # Initialise and train meta_model\n",
    "    mc = MetaCAT(tokenizer=tokenizer, embeddings=None, config=config)\n",
    "    results = mc.train_from_json(mctrainer_export_path, save_dir_path=save_dir_path)\n",
    "\n",
    "    # Save results\n",
    "    json.dump(results, open(os.path.join(save_dir_path,'meta_'+meta_model+'_results.json'), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_model in meta_model_names:\n",
    "    # load and sort out the config\n",
    "    config_file = os.path.join(base_dir_meta_models,\"meta_\"+meta_model,\"config.json\")\n",
    "    with open(config_file, 'r') as jfile:\n",
    "        config_dict = json.load(jfile)\n",
    "    config = ConfigMetaCAT()\n",
    "    for key, value in config_dict.items():\n",
    "        setattr(config, key, value['py/state']['__dict__'])\n",
    "\n",
    "    # change model name if training BERT for the first time\n",
    "    config['model']['model_name'] = 'bert'\n",
    "\n",
    "    # change input_size as well\n",
    "    config['model']['input_size'] = 768\n",
    "\n",
    "    tokenizer = TokenizerWrapperBERT.load(os.path.join(base_dir_meta_models,\"meta_\"+meta_model), config['model']['model_variant'])\n",
    "    \n",
    "    save_dir_path= \"test_meta_\"+meta_model # Where to save the meta_model and results. \n",
    "    #Ideally this should replace the meta_models inside the modelpack\n",
    "\n",
    "    # Initialise and train meta_model\n",
    "    mc = MetaCAT(tokenizer=tokenizer, embeddings=None, config=config)\n",
    "    results = mc.train_from_json(mctrainer_export_path, save_dir_path=save_dir_path)\n",
    "\n",
    "    # Save results\n",
    "    json.dump(results, open(os.path.join(save_dir_path,'meta_'+meta_model+'_results.json'), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you dont have the model packs, and are training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigMetaCAT()\n",
    "# make sure to change the following parameters:\n",
    "# config['model']['nclasses']\n",
    "# config['general']['category_name']\n",
    "\n",
    "# change model name if training BERT for the first time\n",
    "config['model']['model_name'] = 'bert'\n",
    "\n",
    "tokenizer = TokenizerWrapperBERT.load(\"\", config['model']['model_variant'])\n",
    "\n",
    "save_dir_path= \"test_meta\" # Where to save the meta_model and results. \n",
    "#Ideally this should replace the meta_models inside the modelpack\n",
    "\n",
    "# Initialise and train meta_model\n",
    "mc = MetaCAT(tokenizer=tokenizer, embeddings=None, config=config)\n",
    "results = mc.train_from_json(mctrainer_export_path, save_dir_path=save_dir_path)\n",
    "\n",
    "# Save results\n",
    "json.dump(results, open(os.path.join(save_dir_path,'meta_results.json'), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If using 2 phase learning with training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow same steps till defining save_dir_path\n",
    "\n",
    "######################################################################################################\n",
    "# 2 phase learning (used for imbalanced datasets) - trains the models twice: \n",
    "#                    phase 1: trains for minority class(es) by undersampling data\n",
    "#                    phase 2: trains for all classes\n",
    "# parameter values: \n",
    "# 1: Phase 1 - Train model on undersampled data\n",
    "# 2: Phase 2 - Continue training on full data\n",
    "# 0: None\n",
    "\n",
    "# NOTE: Make sure to use class weights in favour of minority classes with 2 phase learning\n",
    "#####################################################################################################\n",
    "\n",
    "config.model.phase_number = 1\n",
    "\n",
    "# specify the class that will define the desired sample size for the undersampling process\n",
    "# if this is left empty, the class with the lowest samples will be chosen\n",
    "# example shown for Status classification task\n",
    "config.model['category_undersample'] = 'Other'\n",
    "\n",
    "\n",
    "# For class weights\n",
    "# using specified class weights\n",
    "config['train']['class_weights'] = [0.3,0.7]\n",
    "\n",
    "# to calculate class weights based on class distribution\n",
    "config['train']['compute_class_weights'] = True\n",
    "\n",
    "# NOTE: when using class weights, it is recommended to define the category to index mapping to ensure the weights are assigned to the right class\n",
    "config['general']['category_value2id'] = {\"Other\":1,\"Confirmed\":0}\n",
    "\n",
    "\n",
    "# Initialise and train meta_model \n",
    "mc = MetaCAT(tokenizer=tokenizer, embeddings=None, config=config)\n",
    "results = mc.train_from_json(mctrainer_export_path, save_dir_path=save_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 2nd round of training\n",
    "config.model.phase_number = 2\n",
    "# Train phase 2\n",
    "results = mc.train_from_json(mctrainer_export_path, save_dir_path=save_dir_path)\n",
    "\n",
    "json.dump(results, open(os.path.join(save_dir_path,'meta_'+meta_model+'_results.json'), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can oversample data as well to help mitigate class imbalance. <br> Use this code to generated synthetic data using LLM - [link](https://gist.github.com/shubham-s-agarwal/401ef8bf6cbbd66fa0c76a8fbfc1f6c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the training with original + synthetic data\n",
    "# Follow the same steps till initializing the metacat model\n",
    "\n",
    "# Initialise and train meta_model\n",
    "mc = MetaCAT(tokenizer=tokenizer, embeddings=None, config=config)\n",
    "\n",
    "# the format expected is [[['text','of','the','document'], [index of medical entity], \"label\" ],\n",
    "#                ['text','of','the','document'], [index of medical entity], \"label\" ]]\n",
    "\n",
    "synthetic_data_export = [[],[],[]]\n",
    "\n",
    "results = mc.train_from_json(mctrainer_export_path, save_dir_path=save_dir_path,data_oversampled=synthetic_data_export)\n",
    "\n",
    "# Save results\n",
    "json.dump(results, open(os.path.join(save_dir_path,'meta_results.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medcat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e4ccc64ca47f932c34194843713e175cf3a19af3798844e4190152d16ba61ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
